{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9c5656",
   "metadata": {},
   "source": [
    "# Simple RAG System (Patched) — LLM Answer via FLAN-T5 Base\n",
    "\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG)** pipeline and uses a pre-trained LLM (**google/flan-t5-base**) to synthesize final answers from the retrieved context chunks.\n",
    "\n",
    "- Embeddings: Sentence Transformers (preferred), with TF–IDF fallbacks if unavailable.\n",
    "- Vector store: simple **pandas DataFrame**.\n",
    "- Retrieval: cosine similarity on normalized embeddings.\n",
    "- Final answer: **LLM generation** (swapped in for the previous stitcher).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a09287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers available: True\n",
      "scikit-learn TF-IDF available: True\n"
     ]
    }
   ],
   "source": [
    "# Imports & environment detection\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "_SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "_TFIDF_SKLEARN_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except Exception:\n",
    "    _SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    _TFIDF_SKLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    _TFIDF_SKLEARN_AVAILABLE = False\n",
    "\n",
    "print(\"sentence-transformers available:\", _SENTENCE_TRANSFORMERS_AVAILABLE)\n",
    "print(\"scikit-learn TF-IDF available:\", _TFIDF_SKLEARN_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc502434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking utility (word-based with overlap)\n",
    "def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 100) -> List[str]:\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start = max(0, end - chunk_overlap)\n",
    "    return chunks\n",
    "\n",
    "def _normalize(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else (v / n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e8730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding backends\n",
    "class SentenceTransformerEmbedder:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        if not _SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            raise RuntimeError('sentence-transformers not available')\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError((\"Failed to load SentenceTransformer model '{}'\\n\\nEnsure it's cached locally. Original error: {}\").format(model_name, e))\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        return self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "class SklearnTfidfEmbedder:\n",
    "    def __init__(self):\n",
    "        if not _TFIDF_SKLEARN_AVAILABLE:\n",
    "            raise RuntimeError('scikit-learn not available')\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.fitted = False\n",
    "    def fit(self, texts: List[str]):\n",
    "        self.vectorizer.fit(texts)\n",
    "        self.fitted = True\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError('Vectorizer not fitted. Call fit(texts) first.')\n",
    "        mat = self.vectorizer.transform(texts)\n",
    "        arr = mat.toarray()\n",
    "        arr = np.vstack([_normalize(row) for row in arr])\n",
    "        return arr\n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        arr = self.vectorizer.transform([query]).toarray()[0]\n",
    "        return _normalize(arr)\n",
    "\n",
    "class MinimalTfidfEmbedder:\n",
    "    def __init__(self):\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.idf: Optional[np.ndarray] = None\n",
    "        self.fitted = False\n",
    "    @staticmethod\n",
    "    def _tokenize(text: str) -> List[str]:\n",
    "        return [t.lower() for t in text.split()]\n",
    "    def fit(self, texts: List[str]):\n",
    "        tokenized = [self._tokenize(t) for t in texts]\n",
    "        vocab = {}\n",
    "        for doc in tokenized:\n",
    "            for tok in set(doc):\n",
    "                if tok not in vocab:\n",
    "                    vocab[tok] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        df = np.zeros(len(vocab), dtype=float)\n",
    "        for doc in tokenized:\n",
    "            seen = set(doc)\n",
    "            for tok in seen:\n",
    "                df[self.vocab[tok]] += 1.0\n",
    "        N = float(len(tokenized))\n",
    "        self.idf = np.log((N + 1.0) / (df + 1.0)) + 1.0\n",
    "        self.fitted = True\n",
    "    def _tfidf_vec(self, text: str) -> np.ndarray:\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError('Embedder not fitted')\n",
    "        vec = np.zeros(len(self.vocab), dtype=float)\n",
    "        toks = self._tokenize(text)\n",
    "        counts = {}\n",
    "        for t in toks:\n",
    "            if t in self.vocab:\n",
    "                counts[t] = counts.get(t, 0) + 1\n",
    "        if not counts:\n",
    "            return vec\n",
    "        total = sum(counts.values())\n",
    "        for t, c in counts.items():\n",
    "            idx = self.vocab[t]\n",
    "            tf = c / total\n",
    "            vec[idx] = tf * (self.idf[idx] if self.idf is not None else 1.0)\n",
    "        return _normalize(vec)\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        return np.vstack([self._tfidf_vec(t) for t in texts])\n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        return self._tfidf_vec(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567ada66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Retrieval helper functions (top-1 to top-2 chunks) ===\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def retrieve_top_chunks(rag, query: str, top_k: int = 2) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant text chunks for a user query using cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        rag: An instance of RAGSystem with a built KB (rag.kb_df is not None).\n",
    "        query: The user's query string.\n",
    "        top_k: Number of chunks to return (1–2 recommended for concise answers).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - 'chunk_id': int\n",
    "            - 'score': float (cosine similarity)\n",
    "            - 'text': str (chunk content)\n",
    "    \"\"\"\n",
    "    if rag.kb_df is None or len(rag.kb_df) == 0:\n",
    "        raise RuntimeError('KB not built yet. Call rag.build_kb(file_path) first.')\n",
    "    if not isinstance(top_k, int) or top_k < 1:\n",
    "        raise ValueError('top_k must be a positive integer.')\n",
    "\n",
    "    q_vec = rag._embed_query(query)\n",
    "    A = np.vstack(rag.kb_df['embedding'].values)\n",
    "    sims = A @ q_vec\n",
    "    top_idx = np.argsort(-sims)[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        row = rag.kb_df.iloc[i]\n",
    "        results.append({\n",
    "            'chunk_id': int(row['chunk_id']),\n",
    "            'score': float(sims[i]),\n",
    "            'text': row['text']\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def retrieve_context(rag, query: str, top_k: int = 2, sep: str = \"\\n\\n\") -> str:\n",
    "    \"\"\"Return a single context string made from the top retrieved chunks.\"\"\"\n",
    "    results = retrieve_top_chunks(rag, query, top_k=top_k)\n",
    "    return sep.join(r[\"text\"] for r in results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961b9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Build final RAG prompt (query + retrieved context) ===\n",
    "def build_rag_prompt(query: str, context_chunks: list, max_words: int = 600) -> str:\n",
    "    \"\"\"\n",
    "    Construct an instruction-style prompt for an LLM from query + retrieved context.\n",
    "    - context_chunks: list[str] (top retrieved chunk texts)\n",
    "    - max_words: approximate cap to keep prompt within model input length\n",
    "    \"\"\"\n",
    "    def truncate_words(text: str, limit: int) -> str:\n",
    "        words = text.split()\n",
    "        return \" \".join(words[:limit]) if len(words) > limit else text\n",
    "\n",
    "    # Join chunk texts with explicit newlines and cap length safely\n",
    "    context = \"\\n\\n\".join(ch for ch in context_chunks if isinstance(ch, str) and ch.strip())\n",
    "    context = truncate_words(context, max_words)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Respond in English.\\n\\n\"\n",
    "        \"Answer the user's question using ONLY the provided context. \"\n",
    "        \"If the answer is not in the context, say: I don't know.\\n\\n\"\n",
    "        + \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        + \"Question:\\n\" + query + \"\\n\\n\"\n",
    "        + \"Answer:\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55032fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === LLM generation via Hugging Face (default: google/flan-t5-base) ===\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoConfig\n",
    "    _TRANSFORMERS_AVAILABLE = True\n",
    "except Exception:\n",
    "    _TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "def generate_llm_answer(\n",
    "    prompt: str,\n",
    "    model_name: str = \"google/flan-t5-base\",\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run inference with FLAN-T5 base.\n",
    "    - Truncates long inputs to the tokenizer's max length (typically 512).\n",
    "    - Uses beam search and anti-repetition constraints to avoid loops.\n",
    "    \"\"\"\n",
    "    if not _TRANSFORMERS_AVAILABLE:\n",
    "        return \"(Transformers unavailable)\\n\" + prompt\n",
    "\n",
    "    try:\n",
    "        cfg = AutoConfig.from_pretrained(model_name)\n",
    "        is_enc_dec = getattr(cfg, \"is_encoder_decoder\", False)\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Respect tokenizer max length (commonly 512 for T5)\n",
    "        max_len = getattr(tok, \"model_max_length\", 512)\n",
    "        if not isinstance(max_len, int) or max_len <= 0 or max_len > 100000:\n",
    "            max_len = 512  # defensive default\n",
    "\n",
    "        if is_enc_dec:\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            inputs = tok(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_len\n",
    "            )\n",
    "            gen = mdl.generate(\n",
    "                **inputs,\n",
    "                max_new_token.0\n",
    "                    3\n",
    "                    30\n",
    "                    3s=max_new_tokens,\n",
    "                # Decoding constraints\n",
    "                do_sample=(temperature > 0.0),\n",
    "                temperature=temperature,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.2,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "            return tok.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "        # (If you swap to a causal model later)\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        inputs = tok(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        gen = mdl.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0.0),\n",
    "            temperature=temperature,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.2,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "        return tok.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"(Model load/inference failed: {e})\\n\" + prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667517aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG system using pandas DataFrame as vector store (LLM-based answer)\n",
    "class RAGSystem:\n",
    "    def __init__(self, embedder: str = 'auto'):\n",
    "        self.embedder_type = embedder\n",
    "        self.embedder = None\n",
    "        self.kb_df: Optional[pd.DataFrame] = None\n",
    "        self._init_embedder(embedder)\n",
    "\n",
    "    def _init_embedder(self, embedder: str):\n",
    "        if embedder == 'sentence-transformer':\n",
    "            self.embedder = SentenceTransformerEmbedder()\n",
    "        elif embedder == 'sklearn-tfidf':\n",
    "            self.embedder = SklearnTfidfEmbedder()\n",
    "        elif embedder == 'minimal-tfidf':\n",
    "            self.embedder = MinimalTfidfEmbedder()\n",
    "        elif embedder == 'auto':\n",
    "            try:\n",
    "                if _SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                    self.embedder = SentenceTransformerEmbedder()\n",
    "                    return\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if _TFIDF_SKLEARN_AVAILABLE:\n",
    "                    self.embedder = SklearnTfidfEmbedder()\n",
    "                    return\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.embedder = MinimalTfidfEmbedder()\n",
    "        else:\n",
    "            raise ValueError('Unknown embedder type')\n",
    "\n",
    "    def build_kb(self, file_path: str, chunk_size: int = 500, chunk_overlap: int = 100) -> pd.DataFrame:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f'Knowledge base file not found: {file_path}')\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        chunks = chunk_text(content, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        if not chunks:\n",
    "            raise ValueError('No chunks generated; the document might be empty.')\n",
    "        if isinstance(self.embedder, SklearnTfidfEmbedder):\n",
    "            self.embedder.fit(chunks)\n",
    "        if isinstance(self.embedder, MinimalTfidfEmbedder):\n",
    "            self.embedder.fit(chunks)\n",
    "        embs = self.embedder.encode(chunks)\n",
    "        df = pd.DataFrame({\n",
    "            'chunk_id': list(range(len(chunks))),\n",
    "            'text': chunks,\n",
    "            'embedding': list(embs)\n",
    "        })\n",
    "        self.kb_df = df\n",
    "        return df\n",
    "\n",
    "    def _embed_query(self, query: str) -> np.ndarray:\n",
    "        if isinstance(self.embedder, SentenceTransformerEmbedder):\n",
    "            return self.embedder.encode([query])[0]\n",
    "        elif isinstance(self.embedder, SklearnTfidfEmbedder):\n",
    "            return self.embedder.encode_query(query)\n",
    "        elif isinstance(self.embedder, MinimalTfidfEmbedder):\n",
    "            return self.embedder.encode_query(query)\n",
    "        else:\n",
    "            raise RuntimeError('Unknown embedder type')\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_similarity_matrix(A: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        return A @ b\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> pd.DataFrame:\n",
    "        if self.kb_df is None or len(self.kb_df) == 0:\n",
    "            raise RuntimeError('KB not built yet. Call build_kb(file_path) first.')\n",
    "        q = self._embed_query(query)\n",
    "        A = np.vstack(self.kb_df['embedding'].values)\n",
    "        sims = self._cosine_similarity_matrix(A, q)\n",
    "        top_idx = np.argsort(-sims)[:top_k]\n",
    "        result = self.kb_df.iloc[top_idx].copy()\n",
    "        result['score'] = sims[top_idx]\n",
    "        return result\n",
    "\n",
    "    def answer(self, query: str, top_k: int = 3, context_max_words: int = 600, model_name: str = 'google/flan-t5-base', temperature: float = 0.0) -> Tuple[str, pd.DataFrame]:\n",
    "        \"\"\"Generate an LLM-based answer using retrieved context chunks.\n",
    "        Returns (llm_answer, retrieved_df).\n",
    "        \"\"\"\n",
    "        retrieved = self.retrieve(query, top_k=top_k)\n",
    "        context_chunks = retrieved['text'].tolist()\n",
    "        prompt = build_rag_prompt(query, context_chunks, max_words=context_max_words)\n",
    "        llm_answer = generate_llm_answer(prompt, model_name=model_name, max_new_tokens=256, temperature=temperature)\n",
    "        return llm_answer, retrieved[['chunk_id', 'score', 'text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27845166",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set your file path and chunking parameters here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6300a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file: pikachu_detailed_article.txt\n"
     ]
    }
   ],
   "source": [
    "# Point to your knowledge base file\n",
    "FILE_PATH = 'pikachu_detailed_article.txt'  # change to your file\n",
    "CHUNK_SIZE = 40\n",
    "CHUNK_OVERLAP = 10\n",
    "EMBEDDER = 'auto'\n",
    "\n",
    "# Helper: create a small sample doc if none exists (for demo only)\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    sample = (\n",
    "        'Pikachu is an Electric-type Pokémon known for its ability to store electricity in its cheeks. '\n",
    "        'Its signature move is Thunderbolt, and it often uses Quick Attack for agility. '\n",
    "        'Pikachu evolves from Pichu via friendship and can evolve into Raichu when exposed to a Thunder Stone. '\n",
    "        'It prefers forests and grassy plains, where it forages and interacts with its group. '\n",
    "        'When threatened, Pikachu releases bursts of electricity to deter predators. '\n",
    "        'Trainers often focus on speed and special attack builds to maximize battle effectiveness.'\n",
    "    )\n",
    "    with open(FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(sample)\n",
    "    print('Created sample file:', FILE_PATH)\n",
    "else:\n",
    "    print('Using existing file:', FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62a19e",
   "metadata": {},
   "source": [
    "## Build the Knowledge Base\n",
    "This step reads the file, chunks the content, computes embeddings, and stores them in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d602b1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB chunks: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Pikachu is one of the most recognizable Pokémo...</td>\n",
       "      <td>[-0.03211138, 0.02871644, -0.042600363, 0.0217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pikachu was intended to be a cute and approach...</td>\n",
       "      <td>[-0.038644757, 0.022668956, -0.023038378, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pikachu evolves from Pichu when it reaches a h...</td>\n",
       "      <td>[-0.03887519, 0.009050059, -0.029975615, 0.069...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the Pokémon evolutionary chain. The design of ...</td>\n",
       "      <td>[-0.06461291, 0.03256252, 0.02205632, 0.040416...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>As the franchise grew, Pikachu's design became...</td>\n",
       "      <td>[0.0045826556, 0.039224394, 0.012920311, -0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id                                               text  \\\n",
       "0         0  Pikachu is one of the most recognizable Pokémo...   \n",
       "1         1  Pikachu was intended to be a cute and approach...   \n",
       "2         2  Pikachu evolves from Pichu when it reaches a h...   \n",
       "3         3  the Pokémon evolutionary chain. The design of ...   \n",
       "4         4  As the franchise grew, Pikachu's design became...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.03211138, 0.02871644, -0.042600363, 0.0217...  \n",
       "1  [-0.038644757, 0.022668956, -0.023038378, 0.06...  \n",
       "2  [-0.03887519, 0.009050059, -0.029975615, 0.069...  \n",
       "3  [-0.06461291, 0.03256252, 0.02205632, 0.040416...  \n",
       "4  [0.0045826556, 0.039224394, 0.012920311, -0.01...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = RAGSystem(embedder=EMBEDDER)\n",
    "kb_df = rag.build_kb(FILE_PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "print('KB chunks:', len(kb_df))\n",
    "kb_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23978b44",
   "metadata": {},
   "source": [
    "## Ask (LLM-powered)\n",
    "Use `rag.answer(query, top_k=2)` to retrieve context and synthesize an answer via FLAN-T5 base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ad1050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Answer ===\n",
      "when it reaches a high level of friendship\n"
     ]
    }
   ],
   "source": [
    "def ask_llm(query: str, top_k: int = 2):\n",
    "    answer, docs = rag.answer(query, top_k=top_k, context_max_words=100, model_name='google/flan-t5-base')\n",
    "    print('=== Retrieved Context (top chunks) ===')\n",
    "    for _, row in docs.iterrows():\n",
    "        print(f\"[chunk {row['chunk_id']}] score={row['score']:.4f}\\n{row['text'][:400]}\\n\")\n",
    "    print('=== LLM Answer ===')\n",
    "    print(answer)\n",
    "\n",
    "# Example question\n",
    "ask_llm('How does Pikachu evolve?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b5a221c-4f98-4498-8fbd-ef165f061248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Answer ===\n",
      "when it reaches a high level of friendship\n",
      "=== LLM Answer ===\n",
      "I don't know\n",
      "=== LLM Answer ===\n",
      "Pikachu became a cultural icon.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ask_llm(\"How does Pikachu evolve?\", top_k=2)\n",
    "ask_llm(\"What is Pikachu's base Speed stat in the original games?\", top_k=2)\n",
    "ask_llm(\"In two sentences, summarize Pikachu's role in the games and anime.\", top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb40257-7fb1-4932-8aff-aa5916b2633c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
